import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from scipy.stats import shapiro

def regression_analysis_page():
    st.title("üìä Analyse de r√©gression")

    # Charger directement le fichier CSV
    file_path = "data/final_filtered_data_sample.csv"
    try:
        filtered_data = pd.read_csv(file_path)

        # √âtapes pr√©liminaires
        st.header("√âtapes pr√©liminaires")
        st.markdown(
            """
            Apr√®s notre √©puration des donn√©es, nous nous retrouvons avec un √©chantillon de 1397 communes 
            pour lesquelles nous avons un jeu de donn√©es complet, au lieu des 1627 communes que compte le PVD au total.

            Nous avons choisi comme variable de r√©ponse le **taux d‚Äô√©volution annuel de la population**.
            Avant de construire le mod√®le, nous avons cr√©√© une **matrice de corr√©lation** comportant toutes nos variables explicatives. 
            Toute variable qui aurait une corr√©lation avec une autre sup√©rieure √† 0,85 serait retir√©e des observations 
            afin d‚Äô√©viter au maximum le risque de multicolin√©arit√©. Lors de cette √©tape, **aucune variable explicative n‚Äôa √©t√© retir√©e**.
            """
        )

        # Affichage des premi√®res lignes
        st.header("Aper√ßu des donn√©es")
        st.write(filtered_data.head())

        # Suppression des colonnes inutiles
        data_for_regression = filtered_data.drop(columns=[
            'taux_evolution_due_solde_naturel', 'taux_evolution_due_solde_migratoire', 'code_insee'
        ])

        # Calcul de la matrice de corr√©lation
        correlation_matrix = data_for_regression.corr()

        # Identification des variables fortement corr√©l√©es
        highly_correlated = np.where((correlation_matrix > 0.85) & (correlation_matrix != 1))
        correlated_pairs = [
            (correlation_matrix.index[x], correlation_matrix.columns[y])
            for x, y in zip(*highly_correlated) if x < y
        ]

        # Affichage de la matrice de corr√©lation
        st.header("Matrice de corr√©lation")
        fig, ax = plt.subplots(figsize=(12, 10))
        cax = ax.imshow(correlation_matrix, cmap='coolwarm', interpolation='nearest')
        fig.colorbar(cax)
        ax.set_title("Matrice de corr√©lation des variables")
        st.pyplot(fig)

        # Affichage des paires fortement corr√©l√©es
        if correlated_pairs:
            st.subheader("Paires de variables fortement corr√©l√©es")
            for pair in correlated_pairs:
                st.write(f"{pair[0]} et {pair[1]}")

        # Construction du mod√®le
        st.header("Construction du mod√®le")
        st.markdown(
            """
            Au vu du grand nombre de pr√©dicteurs, nous nous doutions que certains d‚Äôentre eux ne seraient pas significatifs, 
            nous avons donc proc√©d√© √† une **s√©lection backward**. Cela signifie que nous avons commenc√© par g√©n√©rer un mod√®le 
            comportant toutes les variables explicatives pour ensuite retirer une √† une celles ayant la plus grande p-value sup√©rieure au seuil de 0,05.

            Cette s√©lection a permis de retirer 28 des 41 variables que nous avions choisies, faisant passer l‚ÄôAkaike Information Criterion (AIC) 
            du mod√®le de **3067 √† 3035**.
            """
        )

        # S√©paration des variables pr√©dicteurs et r√©ponse
        X = data_for_regression.drop(columns=['taux_evolution'])
        y = data_for_regression['taux_evolution']

        # Ajout d'une constante aux pr√©dicteurs
        X = sm.add_constant(X)

        # Fonction pour r√©gression par √©limination arri√®re avec logs
        def backward_regression_with_logging(X, y, significance_level=0.05):
            steps = []
            step_count = 1
            while True:
                model = sm.OLS(y, X).fit()
                aic = model.aic
                p_values = model.pvalues
                max_p_value = p_values.max()
                if max_p_value > significance_level:
                    excluded_feature = p_values.idxmax()
                    steps.append(f"Step {step_count}: Suppression de {excluded_feature}, AIC: {aic:.2f}")
                    X = X.drop(columns=[excluded_feature])
                    step_count += 1
                else:
                    break
            return model, steps

        # Lancer la r√©gression par √©limination arri√®re
        final_model, regression_steps = backward_regression_with_logging(X, y)

        # Affichage des √©tapes
        st.subheader("√âtapes de la r√©gression")
        for step in regression_steps:
            st.write(step)

        # R√©sum√© du mod√®le final
        st.subheader("R√©sum√© du mod√®le final")
        st.text(final_model.summary())

        # Interpr√©tation des r√©sultats
        st.header("Interpr√©tation des r√©sultats")
        st.markdown(
            """
            - **B0** : La constante du mod√®le est de **-2,6937**, indiquant une valeur de base lorsque toutes les autres variables sont nulles.
            - **Variables significatives (Bj)** :
                - Nombre de cin√©mas : Positivement associ√© (**+0,1088**).
                - Nombre de conservatoires : N√©gativement associ√© (**-1,5047**).
                - Nombre de mus√©es : Impact n√©gatif (**-0,2256**).
                - M√©diane du revenu : Positivement associ√© (**+0,0001**).
                - Part des moins de 15 ans : Effet positif (**+0,0914**).
                - Part des 15-19 ans : Effet n√©gatif (**-0,0382**).
                - Part des 30-44 ans : Impact positif (**+0,0509**).
                - Part des 45-59 ans : Effet n√©gatif (**-0,0678**).
                - Nombre de sup√©rettes et √©piceries : Positivement associ√© (**+0,0242**).
                - Nombre d‚Äô√©coles primaires : Effet n√©gatif (**-0,0603**).
                - Nombre de m√©decins g√©n√©ralistes : Positivement associ√© (**+0,0185**).
                - Nombre de dentistes : Effet positif (**+0,0253**).
                - Nombre de festivals : Impact positif (**+0,0380**).

            - **R¬≤** : Le R¬≤ du mod√®le est de **0,35**, ce qui indique que notre mod√®le n‚Äôexplique qu‚Äôenviron 35 % de la variance du taux d‚Äô√©volution de la d√©mographie des communes du PVD.

            - **F-test** : La p-value pour le test de significativit√© du mod√®le est extr√™mement faible, confirmant que le mod√®le est significatif.
            """
        )

        # Analyse des r√©sidus
        st.header("Analyse des r√©sidus")
        st.markdown(
            """
            - **Homosc√©dasticit√©** : Les r√©sidus ont une moyenne nulle ainsi qu‚Äôune variance constante et ind√©pendante, validant l‚Äôutilisation des tests de nullit√© sur les coefficients et le F-test.
            - **Normalit√© des r√©sidus** : Le QQ plot montre que les r√©sidus ne suivent pas une distribution normale.
            """
        )

        residuals = final_model.resid
        fitted = final_model.fittedvalues

        # R√©sidus vs valeurs ajust√©es
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.scatter(fitted, residuals, alpha=0.6)
        ax.axhline(0, color='red', linestyle='--')
        ax.set_title("R√©sidus vs Valeurs ajust√©es")
        ax.set_xlabel("Valeurs ajust√©es")
        ax.set_ylabel("R√©sidus")
        st.pyplot(fig)

        # QQ Plot
        st.subheader("QQ Plot (Normalit√© des r√©sidus)")
        qq_fig = sm.qqplot(residuals, line='45', fit=True)
        st.pyplot(qq_fig)

        # Explication sur la faible valeur de R¬≤
        st.header("Discussion sur le R¬≤")
        st.markdown(
            """
            La valeur relativement faible du R¬≤ (**0,35**) indique que notre mod√®le explique uniquement 35 % de la variance du taux d‚Äô√©volution d√©mographique. 
            Cela s‚Äôexplique par la complexit√© de cette variable, qui est influenc√©e par un grand nombre de facteurs tr√®s divers. 

            Ici, nous avons pris en compte la **proximit√© des grandes villes**, mais nous n‚Äôavons pas inclus d‚Äôautres types de facteurs environnementaux comme 
            la **proximit√© de la mer**, des **stations de ski**, ou encore des **caract√©ristiques climatiques**. En ajoutant ces variables, 
            ainsi que des facteurs politiques (par exemple, les investissements publics locaux) et historiques, 
            nous pourrions esp√©rer un mod√®le avec un R¬≤ plus √©lev√© et une meilleure explication des variations observ√©es.
            """
        )

    except FileNotFoundError:
        st.error(f"Le fichier sp√©cifi√© √† '{file_path}' est introuvable. Veuillez v√©rifier le chemin.")